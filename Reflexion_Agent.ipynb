{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/hV/KqBPBsNcJxhYAeyoD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammad-BILAL-5/Reflection-Agents/blob/main/Reflexion_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTRUaUQWiNf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8dabd7d-1612-423b-8504-0eb3922ea9bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade langchain langchain_community  langchain-google-genai langgraph tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"langchai_api_key\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"Gemini_Api_Key\")\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"tavily_api_key\")"
      ],
      "metadata": {
        "id": "lLSvYBodi-lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-1.5-pro\")\n",
        "\n",
        "search = TavilySearchAPIWrapper()\n",
        "tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)"
      ],
      "metadata": {
        "id": "sJ5Z4tuhjE8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, ToolMessage\n",
        "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from pydantic import ValidationError\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Reflection(BaseModel):\n",
        "    missing: str = Field(description=\"Critique of what is missing.\")\n",
        "    superfluous: str = Field(description=\"Critique of what is superfluous\")\n",
        "\n",
        "\n",
        "class AnswerQuestion(BaseModel):\n",
        "    \"\"\"Answer the question. Provide an answer, reflection, and then follow up with search queries to improve the answer.\"\"\"\n",
        "\n",
        "    answer: str = Field(description=\"~250 word detailed answer to the question.\")\n",
        "    reflection: Reflection = Field(description=\"Your reflection on the initial answer.\")\n",
        "    search_queries: list[str] = Field(\n",
        "        description=\"1-3 search queries for researching improvements to address the critique of your current answer.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class ResponderWithRetries:\n",
        "    def __init__(self, runnable, validator):\n",
        "        self.runnable = runnable\n",
        "        self.validator = validator\n",
        "\n",
        "    def respond(self, state: dict):\n",
        "        response = []\n",
        "        for attempt in range(3):\n",
        "            response = self.runnable.invoke(\n",
        "                {\"messages\": state[\"messages\"]}, {\"tags\": [f\"attempt:{attempt}\"]}\n",
        "            )\n",
        "            try:\n",
        "                self.validator.invoke(response)\n",
        "                return {\"messages\": response}\n",
        "            except ValidationError as e:\n",
        "                state = state + [\n",
        "                    response,\n",
        "                    ToolMessage(\n",
        "                        content=f\"{repr(e)}\\n\\nPay close attention to the function schema.\\n\\n\"\n",
        "                        + self.validator.schema_json()\n",
        "                        + \" Respond by fixing all validation errors.\",\n",
        "                        tool_call_id=response.tool_calls[0][\"id\"],\n",
        "                    ),\n",
        "                ]\n",
        "        return {\"messages\": response}\n",
        "import datetime\n",
        "\n",
        "actor_prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are expert researcher.\n",
        "Current time: {time}\n",
        "\n",
        "1. {first_instruction}\n",
        "2. Reflect and critique your answer. Be severe to maximize improvement.\n",
        "3. Recommend search queries to research information and improve your answer.\"\"\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        (\n",
        "            \"user\",\n",
        "            \"\\n\\n<system>Reflect on the user's original question and the\"\n",
        "            \" actions taken thus far. Respond using the {function_name} function.</reminder>\",\n",
        "        ),\n",
        "    ]\n",
        ").partial(\n",
        "    time=lambda: datetime.datetime.now().isoformat(),\n",
        ")\n",
        "initial_answer_chain = actor_prompt_template.partial(\n",
        "    first_instruction=\"Provide a detailed ~250 word answer.\",\n",
        "    function_name=AnswerQuestion.__name__,\n",
        ") | llm.bind_tools(tools=[AnswerQuestion])\n",
        "validator = PydanticToolsParser(tools=[AnswerQuestion])\n",
        "\n",
        "first_responder = ResponderWithRetries(\n",
        "    runnable=initial_answer_chain, validator=validator\n",
        ")\n",
        "example_question = \"Why is reflection useful in AI?\"\n",
        "initial = first_responder.respond(\n",
        "    {\"messages\": [HumanMessage(content=example_question)]}\n",
        ")"
      ],
      "metadata": {
        "id": "nv2dVJLGjJ4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(initial)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NQUCjqi97HP",
        "outputId": "558fc179-a713-49e5-974f-34eb307264b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': AIMessage(content='', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'MALFORMED_FUNCTION_CALL', 'safety_ratings': []}, id='run-21393c67-9786-41ed-a748-c4c0737237c2-0', usage_metadata={'input_tokens': 287, 'output_tokens': 0, 'total_tokens': 287, 'input_token_details': {'cache_read': 0}})}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "revise_instructions = \"\"\"Revise your previous answer using the new information.\n",
        "    - You should use the previous critique to add important information to your answer.\n",
        "        - You MUST include numerical citations in your revised answer to ensure it can be verified.\n",
        "        - Add a \"References\" section to the bottom of your answer (which does not count towards the word limit). In form of:\n",
        "            - [1] https://example.com\n",
        "            - [2] https://example.com\n",
        "    - You should use the previous critique to remove superfluous information from your answer and make SURE it is not more than 250 words.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Extend the initial answer schema to include references.\n",
        "# Forcing citation in the model encourages grounded responses\n",
        "class ReviseAnswer(AnswerQuestion):\n",
        "    \"\"\"Revise your original answer to your question. Provide an answer, reflection,\n",
        "\n",
        "    cite your reflection with references, and finally\n",
        "    add search queries to improve the answer.\"\"\"\n",
        "\n",
        "    references: list[str] = Field(\n",
        "        description=\"Citations motivating your updated answer.\"\n",
        "    )\n",
        "\n",
        "\n",
        "revision_chain = actor_prompt_template.partial(\n",
        "    first_instruction=revise_instructions,\n",
        "    function_name=ReviseAnswer.__name__,\n",
        ") | llm.bind_tools(tools=[ReviseAnswer])\n",
        "revision_validator = PydanticToolsParser(tools=[ReviseAnswer])\n",
        "\n",
        "revisor = ResponderWithRetries(runnable=revision_chain, validator=revision_validator)\n",
        "import json\n",
        "\n",
        "revised = revisor.respond(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=example_question),\n",
        "            initial[\"messages\"],\n",
        "            ToolMessage(\n",
        "                tool_call_id=initial[\"messages\"].tool_calls[0][\"id\"],\n",
        "                content=json.dumps(\n",
        "                    tavily_tool.invoke(\n",
        "                        {\n",
        "                            \"query\": initial[\"messages\"].tool_calls[0][\"args\"][\n",
        "                                \"search_queries\"\n",
        "                            ][0]\n",
        "                        }\n",
        "                    )\n",
        "                ),\n",
        "            ),\n",
        "        ]\n",
        "    }\n",
        ")\n",
        "revised[\"messages\"]"
      ],
      "metadata": {
        "id": "pJW0v0gWjJbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import StructuredTool\n",
        "\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "\n",
        "def run_queries(search_queries: list[str], **kwargs):\n",
        "    \"\"\"Run the generated queries.\"\"\"\n",
        "    return tavily_tool.batch([{\"query\": query} for query in search_queries])\n",
        "\n",
        "\n",
        "tool_node = ToolNode(\n",
        "    [\n",
        "        StructuredTool.from_function(run_queries, name=AnswerQuestion.__name__),\n",
        "        StructuredTool.from_function(run_queries, name=ReviseAnswer.__name__),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "TB4buI-1jQkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "MAX_ITERATIONS = 5\n",
        "builder = StateGraph(State)\n",
        "builder.add_node(\"draft\", first_responder.respond)\n",
        "\n",
        "\n",
        "builder.add_node(\"execute_tools\", tool_node)\n",
        "builder.add_node(\"revise\", revisor.respond)\n",
        "# draft -> execute_tools\n",
        "builder.add_edge(\"draft\", \"execute_tools\")\n",
        "# execute_tools -> revise\n",
        "builder.add_edge(\"execute_tools\", \"revise\")\n",
        "\n",
        "# Define looping logic:\n",
        "\n",
        "\n",
        "def _get_num_iterations(state: list):\n",
        "    i = 0\n",
        "    for m in state[::-1]:\n",
        "        if m.type not in {\"tool\", \"ai\"}:\n",
        "            break\n",
        "        i += 1\n",
        "    return i\n",
        "\n",
        "\n",
        "def event_loop(state: list):\n",
        "    # in our case, we'll just stop after N plans\n",
        "    num_iterations = _get_num_iterations(state[\"messages\"])\n",
        "    if num_iterations > MAX_ITERATIONS:\n",
        "        return END\n",
        "    return \"execute_tools\"\n",
        "\n",
        "\n",
        "# revise -> execute_tools OR end\n",
        "builder.add_conditional_edges(\"revise\", event_loop, [\"execute_tools\", END])\n",
        "builder.add_edge(START, \"draft\")\n",
        "graph = builder.compile()\n",
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "id": "SI9UIM1VjUAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events = graph.stream(\n",
        "    {\"messages\": [(\"user\", \"How should we handle the climate crisis?\")]},\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for i, step in enumerate(events):\n",
        "    print(f\"Step {i}\")\n",
        "    step[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "CtqUncHnjgd7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}